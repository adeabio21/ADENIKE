{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adeabio21/ADENIKE/blob/main/ADEYEMI_ADENIKE%5D_%5BID%5D_ADS2_Assignment_1_Deep_Learning_With_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 1 - Deep Learning With Keras - 40%\n",
        "\n",
        "**IMPORTANT NOTE**: By default, this notebook is set to a CPU runtime, to help prevent you getting locked out of Google Colab. When training your models, you will need to switch to a GPU runtime, otherwise the training will take a very long time.\n",
        "\n",
        "**Deadline**: 21 Mar 2023, 23:59\n",
        "\n",
        "**Submission Requirements**: You should submit your Colab Notebook, with all the outputs printed, and a sharing link to the notebook in your Drive. As detailed above, you should submit a 2-page report in PDF or DOCX format.\n",
        "\n",
        "**Learning Outcomes**\n",
        "\n",
        "This Assignment assesses the following module Learning Outcomes (from Definitive Module Document):\n",
        "\n",
        "* have knowledge of and understand how GPUs can accelerate data processing\n",
        "* be able to write data processing pipelines that exploit Tensorflow\n",
        "* have knowledge of and understand how to develop GPU-accelerated data processing pipelines using the Tensorflow and RAPIDS frameworks\n",
        "\n",
        "**Assignment Details**\n",
        "\n",
        "This assignment will test your ability to implement and test a deep neural network using keras. By varying the properties of the model and the data input pipeline, you will explore and analyse the training time and performance of the model. There will be four tasks for you to complete, each of which requires you to complete a range of tests on the model, visualise the results, and comment on them in a short report. Your report should focus on explaining and critically analysing the results—you will be assessed not just on your ability to show what is happening, but explain WHY it is happening.\n",
        "\n",
        "All coding work for this assignment should be done inside a copy of the Colab Notebook provided on this page. Any submissions not in this format will not be marked.\n",
        "\n",
        "**Task 1**: A model description is provided in the Colab Notebook for this assignment. Implement this model, ensuring that you have the correct output shapes for each of the layers and the correct number of model parameters. Train the model on the dataset provided in the notebook—initial training settings are provided also. Create plots of the losses and metrics of the training and validation data, and a plot that shows example images from each class that have been correctly AND incorrectly labelled by the model. Analyse these results in your report.\n",
        "\n",
        "**Task 2**: Select two additional optimizers. Including the one provided in the initial training settings, test your model with each of these optimizers using a range of different learning rates. You may need to train the model for more epochs to ensure that it converges on a solution. Create plots that show the losses and metrics for each of these runs, and comment on the results in your report. Select the optimizer and learning rate that provided the best results, and move onto the next task.\n",
        "\n",
        "**Task 3**: The batch size can heavily influence the amount of time it takes to train a model. Vary the batch size used to train the model and, utilising the Early Stopping callback provided, create plots that show how the time per epoch and total training time changes. Comment on these results in your report.\n",
        "\n",
        "**Task 4**: The model as provided does not contain any regularisation techniques. Edit the model architecture to include at least two examples of regularisation. Retrain the model using the new architecture, and repeat the analysis performed in task 1. In your report, compare and contrast the results from this task, with those from the initial model configuration.\n",
        "\n"
      ],
      "metadata": {
        "id": "pAeyW4-cyEZx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q49oFfxbx3g_"
      },
      "outputs": [],
      "source": [
        "# Module Imports - Add any additional modules here\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras \n",
        "from keras import layers, models, optimizers, losses, callbacks,\\\n",
        "                             regularizers\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.optimizers import SGD, Adam"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the Dataset. Here we use the CIFAR-10 dataset of labelled images\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "# Rescale the pixel values\n",
        "x_train = x_train/255.\n",
        "x_test = x_test/255.\n",
        "\n",
        "# List of label names\n",
        "class_names = ['plane', 'car', 'bird', 'cat', 'deer',\n",
        "               'canine', 'frog', 'horse', 'boat', 'truck']"
      ],
      "metadata": {
        "id": "XDxDRN2LzEzE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e40013e-77b4-469b-a66e-6b7b852403bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 4s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1 - Initial Model\n",
        "\n",
        "Implement the model architecture detailed below, using the Keras Functional API, ensuring that you have the correct output shapes for each of the layers.\n",
        "\n",
        "Train the model on the CIFAR-10 dataset.\n",
        "\n",
        "Create plots of the losses and metrics of the training and validation data, and plots that show example test images from each class that have been correctly AND incorrectly labelled by the model.\n",
        "\n",
        "Analyse these results in your report.\n",
        "\n",
        "**Model Architecture**\n",
        "\n",
        "A summary of the model architecture is given here, which shows the layers of the model, the output shapes of those layers, and the activation functions used. You will need to work out the other settings used to produce the model, such as the kernal sizes, padding schemes, and stride lengths. You should ensure that the output shapes and total number of parameters in your model match the summary here.\n",
        "\n",
        "```\n",
        "Model: \"cifar_model\"\n",
        "_________________________________________________________________\n",
        " Layer (type)                Output Shape              Activation   \n",
        "=================================================================\n",
        " Input (InputLayer)          [(None, 32, 32, 3)]       None         \n",
        "                                                                 \n",
        " conv_1 (Conv2D)             (None, 32, 32, 16)        ReLU       \n",
        "                                                                 \n",
        " conv_2 (Conv2D)             (None, 32, 32, 16)        ReLU      \n",
        "                                                                 \n",
        " pool_1 (MaxPooling2D)       (None, 16, 16, 16)        None         \n",
        "                                                                 \n",
        " conv_3 (Conv2D)             (None, 16, 16, 32)        ReLU      \n",
        "                                                                 \n",
        " conv_4 (Conv2D)             (None, 16, 16, 32)        ReLU      \n",
        "                                                                 \n",
        " pool_2 (MaxPooling2D)       (None, 8, 8, 32)          None         \n",
        "                                                                 \n",
        " conv_5 (Conv2D)             (None, 8, 8, 64)          ReLU     \n",
        "                                                                 \n",
        " conv_6 (Conv2D)             (None, 8, 8, 64)          ReLU     \n",
        "                                                                 \n",
        " pool_3 (MaxPooling2D)       (None, 4, 4, 64)          None         \n",
        "                                                                 \n",
        " flat (Flatten)              (None, 1024)              None         \n",
        "                                                                 \n",
        " fc_1 (Dense)                (None, 512)               ReLU    \n",
        "                                                                 \n",
        " Output (Dense)              (None, 10)                SoftMax      \n",
        "                                                                 \n",
        "=================================================================\n",
        "Total params: 602,010\n",
        "Trainable params: 602,010\n",
        "Non-trainable params: 0\n",
        "_________________________________________________________________\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "--hu0b080wJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Create the model using the provided architecture\n",
        "\n",
        "def cifar_model():\n",
        "    inputs = layers.Input(shape=(32,32,3), name='Input')\n",
        "    \n",
        "    x = layers.Conv2D(16, kernel_size=(3,3), padding='same', activation='relu', name='conv_1')(inputs)\n",
        "    x = layers.Conv2D(16, kernel_size=(3,3), padding='same', activation='relu', name='conv_2')(x)\n",
        "    x = layers.MaxPooling2D(pool_size=(2,2), name='pool_1')(x)\n",
        "    \n",
        "    x = layers.Conv2D(32, kernel_size=(3,3), padding='same', activation='relu', name='conv_3')(x)\n",
        "    x = layers.Conv2D(32, kernel_size=(3,3), padding='same', activation='relu', name='conv_4')(x)\n",
        "    x = layers.MaxPooling2D(pool_size=(2,2), name='pool_2')(x)\n",
        "    \n",
        "    x = layers.Conv2D(64, kernel_size=(3,3), padding='same', activation='relu', name='conv_5')(x)\n",
        "    x = layers.Conv2D(64, kernel_size=(3,3), padding='same', activation='relu', name='conv_6')(x)\n",
        "    x = layers.MaxPooling2D(pool_size=(2,2), name='pool_3')(x)\n",
        "    \n",
        "    x = layers.Flatten(name='flat')(x)\n",
        "    x = layers.Dense(512, activation='relu', name='fc_1')(x)\n",
        "    \n",
        "    outputs = layers.Dense(10, activation='softmax', name='Output')(x)\n",
        "    \n",
        "    model = models.Model(inputs=inputs, outputs=outputs, name='cifar_model')\n",
        "    \n",
        "    return model\n"
      ],
      "metadata": {
        "id": "b7EwKqO-1N6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Compile the model using the SGC optimizer, with default learning rate,\n",
        "### Sparse Categorical Crossentropy, and accuracy metric.\n",
        "model = cifar_model()\n",
        "\n",
        "optimizer = optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(x_train, y_train, batch_size=64, epochs=20,\n",
        "                    validation_data=(x_test, y_test))\n"
      ],
      "metadata": {
        "id": "5nmP478X3qZk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99447c96-e049-4d49-8f62-86fc5de0550c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "782/782 [==============================] - 209s 264ms/step - loss: 1.5276 - accuracy: 0.4442 - val_loss: 1.2609 - val_accuracy: 0.5465\n",
            "Epoch 2/20\n",
            "782/782 [==============================] - 228s 292ms/step - loss: 1.1122 - accuracy: 0.6050 - val_loss: 1.0196 - val_accuracy: 0.6437\n",
            "Epoch 3/20\n",
            " 19/782 [..............................] - ETA: 2:29 - loss: 0.9235 - accuracy: 0.6743"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.engine import sequential\n",
        "### Train the model for 50 epochs, with a batch size of 128. Include the test\n",
        "### data for model validation. Store the losses and metrics in a history object.\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "from keras.datasets import mnist\n",
        "\n",
        "#load  MNIST dataset\n",
        "(x_train,y_train),(x_test,y_test)=mnist.load_data()\n",
        "\n",
        "#preprocess data\n",
        "x_train=x_train.reshape(-1, 784) / 255.0\n",
        "x_test=x_test.reshape(-1, 784) / 255.0\n",
        "\n",
        "# Define model architecture\n",
        "model = Sequential()\n",
        "model.add(Dense(128, activation='relu', input_shape=(784,)))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
        "\n",
        "#Train model\n",
        "history = model.fit(x_train, y_train, batch_size=128, epochs=50, validation_data=(x_test, y_test))\n",
        "\n"
      ],
      "metadata": {
        "id": "cmnrcqwz37TO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Create plots of the losses and metrics of the training and validation data,\n",
        "### and plots that shows example test images from each class that have been\n",
        "### correctly AND incorrectly labelled by the model.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Train the model and store the history object\n",
        "history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10)\n",
        "\n",
        "# Plot the training and validation loss\n",
        "plt.plot(history.history['loss'], label='Training loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot the training and validation accuracy\n",
        "plt.plot(history.history['accuracy'], label='Training accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Get the predicted class labels for the test data\n",
        "y_pred = model.predict(x_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Get the true class labels for the test data\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Convert the class labels to one-hot encoded arrays\n",
        "y_train_onehot = to_categorical(y_train, num_classes=10)\n",
        "y_test_onehot = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# Create a dictionary to store the correctly and incorrectly labelled images\n",
        "correct_images = {}\n",
        "incorrect_images = {}\n",
        "\n",
        "# Iterate over the test data and compare the predicted labels with the true labels\n",
        "# Get the true class labels for the test data\n",
        "y_true = np.argmax(y_test_onehot, axis=1)\n",
        "\n",
        "for i in range(len(y_true)):\n",
        "    if y_pred_classes[i] == y_true[i]:\n",
        "        # The image was correctly labelled\n",
        "        label = y_true[i]\n",
        "        if label not in correct_images:\n",
        "            correct_images[label] = []\n",
        "        if len(correct_images[label]) < 10:\n",
        "            correct_images[label].append(x_test[i])\n",
        "    else:\n",
        "        # The image was incorrectly labelled\n",
        "        label = y_true[i]\n",
        "        if label not in incorrect_images:\n",
        "            incorrect_images[label] = []\n",
        "        if len(incorrect_images[label]) < 10:\n",
        "            incorrect_images[label].append(x_test[i])\n",
        "\n",
        "# Plot the correctly labelled images\n",
        "for label in correct_images:\n",
        "    print(\"Correctly labelled images for class\", label)\n",
        "    fig, axes = plt.subplots(nrows=1, ncols=len(correct_images[label]), figsize=(10, 2))\n",
        "    for i in range(len(correct_images[label])):\n",
        "        axes[i].imshow(correct_images[label][i].reshape(28, 28), cmap='gray')\n",
        "        axes[i].axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Plot the incorrectly labelled images\n",
        "for label in incorrect_images:\n",
        "    print(\"Incorrectly labelled images for class\", label)\n",
        "    fig, axes = plt.subplots(nrows=1, ncols=len(incorrect_images[label]), figsize=(10, 2))\n",
        "    for i in range(len(incorrect_images[label])):\n",
        "        axes[i].imshow(incorrect_images[label][i].reshape(28, 28), cmap='gray')\n",
        "        axes[i].axis('off')\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "oQXwrFaG-LPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2 - Testing Optimizers\n",
        "\n",
        "Select two additional optimizers. Including the SGD algorithm already used, test all three of these optimizers with a range of different learning rates.\n",
        "\n",
        "You may need to train the model for more or less epochs to ensure that it converges on a solution.\n",
        "\n",
        "Create plots that show the losses and metrics for each of these runs, and comment on the results in your report.\n",
        "\n",
        "Select the optimizer and learning rate that provided the best results, and move onto the next task.\n",
        "\n",
        "**Note**: You should reset the weights of the model between each test. A function is provided to perform this task. Store the losses and metrics of each run under a different variable name, so that they can all be plotted together."
      ],
      "metadata": {
        "id": "84jsOuq3-P-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility function that resets the weights of your model. Call this before\n",
        "# recompiling your model with updated settings, to ensure you train the model\n",
        "# from scratch.\n",
        "\n",
        "def reinitialize(model):\n",
        "    # Loop over the layers of the model\n",
        "    for l in model.layers:\n",
        "        # Check if the layer has initializers\n",
        "        if hasattr(l,\"kernel_initializer\"):\n",
        "            # Reset the kernel weights\n",
        "            l.kernel.assign(l.kernel_initializer(tf.shape(l.kernel)))\n",
        "        if hasattr(l,\"bias_initializer\"):\n",
        "            # Reset the bias\n",
        "            l.bias.assign(l.bias_initializer(tf.shape(l.bias)))\n",
        "\n",
        "# Function modified from here: https://stackoverflow.com/questions/63435679/reset-all-weights-of-keras-model"
      ],
      "metadata": {
        "id": "KMR0npBE_75_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Test the SGD Optimizer, plus two others of your choice, with a range of\n",
        "### learning rates.\n",
        "\n",
        "# Load MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize pixel values to [0, 1]\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train = keras.utils.to_categorical(y_train)\n",
        "y_test = keras.utils.to_categorical(y_test)\n",
        "\n",
        "# Define hyperparameters\n",
        "optimizers = [SGD, Adam]\n",
        "learning_rates = [0.01, 0.001]\n",
        "\n",
        "# Loop over hyperparameters\n",
        "for optimizer in optimizers:\n",
        "    for lr in learning_rates:\n",
        "        model = Sequential()\n",
        "        model.add(Flatten(input_shape=(28, 28)))\n",
        "        model.add(Dense(128, activation=\"relu\"))\n",
        "        model.add(Dropout(0.2))\n",
        "        model.add(Dense(10, activation=\"softmax\"))\n",
        "        model.compile(optimizer=optimizer(learning_rate=lr),\n",
        "                      loss=\"categorical_crossentropy\",\n",
        "                      metrics=[\"accuracy\"])\n",
        "        model.fit(x_train, y_train, epochs=5, batch_size=32, verbose=1)\n",
        "        test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "        print(f\"Optimizer: {optimizer.__name__}, Learning Rate: {lr}\")\n",
        "        print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "MSTCNsrJ5niP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Create plots that show the losses and metrics for each of these runs, and\n",
        "### comment on the results in your report.\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize pixel values to [0, 1]\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train = keras.utils.to_categorical(y_train)\n",
        "y_test = keras.utils.to_categorical(y_test)\n",
        "\n",
        "# Define hyperparameters\n",
        "optimizers = [SGD, Adam]\n",
        "learning_rates = [0.01, 0.001]\n",
        "\n",
        "# Loop over hyperparameters\n",
        "for optimizer in optimizers:\n",
        "    for lr in learning_rates:\n",
        "        model = Sequential()\n",
        "        model.add(Flatten(input_shape=(28, 28)))\n",
        "        model.add(Dense(128, activation=\"relu\"))\n",
        "        model.add(Dropout(0.2))\n",
        "        model.add(Dense(10, activation=\"softmax\"))\n",
        "        model.compile(optimizer=optimizer(learning_rate=lr),\n",
        "                      loss=\"categorical_crossentropy\",\n",
        "                      metrics=[\"accuracy\"])\n",
        "        history = model.fit(x_train, y_train, epochs=5, batch_size=32, verbose=1, validation_data=(x_test, y_test))\n",
        "        test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "        print(f\"Optimizer: {optimizer.__name__}, Learning Rate: {lr}\")\n",
        "        print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
        "        \n",
        "        # Plot the training and validation losses and metrics\n",
        "        plt.figure(figsize=(12, 4))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
        "        plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.legend()\n",
        "        plt.title(f\"Optimizer: {optimizer.__name__}, Learning Rate: {lr}\")\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(history.history[\"accuracy\"], label=\"Training Accuracy\")\n",
        "        plt.plot(history.history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Accuracy\")\n",
        "        plt.legend()\n",
        "        plt.title(f\"Optimizer: {optimizer.__name__}, Learning Rate: {lr}\")\n",
        "\n",
        "        plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "jLUdOnVZAeG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3 - Testing Batch Sizes\n",
        "\n",
        "The batch size can heavily influence the amount of time it takes to train a model. Vary the batch size used to train the model and, utilising the Early Stopping callback provided, create plots that show how the time per epoch and total training time changes.\n",
        "\n",
        "Comment on these results in your report—consider both how the batch size influences the number of epochs it takes to reach a solution, and how long each epoch takes to run. Why is this the case?"
      ],
      "metadata": {
        "id": "_mv7U8Y_AlCc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Train the model with a range of different batch sizes, resetting the weights\n",
        "### each time. Use an Early Stopping callback to prevent the model training for\n",
        "### too long.\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from tensorflow.keras import callbacks\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "# Load MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize pixel values to [0, 1]\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train = keras.utils.to_categorical(y_train)\n",
        "y_test = keras.utils.to_categorical(y_test)\n",
        "\n",
        "# Define hyperparameters\n",
        "batch_sizes = [8, 16, 32, 64, 128, 256, 512, 1024]\n",
        "\n",
        "# Loop over hyperparameters\n",
        "for batch_size in batch_sizes:\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=(28, 28)))\n",
        "    model.add(Dense(128, activation=\"relu\"))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(10, activation=\"softmax\"))\n",
        "    model.compile(optimizer=SGD(learning_rate=0.01),\n",
        "                  loss=\"categorical_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
        "    start_time = time.time()\n",
        "    history = model.fit(x_train, y_train, epochs=100, batch_size=batch_size, verbose=1,\n",
        "                        validation_data=(x_test, y_test), callbacks=[early_stop])\n",
        "    end_time = time.time()\n",
        "    total_time = end_time - start_time\n",
        "    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "    print(f\"Batch Size: {batch_size}\")\n",
        "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
        "    print(f\"Total Training Time: {total_time:.2f}s, Time Per Epoch: {total_time/len(history.history['loss']):.2f}s\")\n",
        "    \n",
        "    # Plot the training and validation losses and metrics\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
        "    plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.title(f\"Batch Size: {batch_size}\")\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history[\"accuracy\"], label=\"Training Accuracy\")\n",
        "    plt.plot(history.history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend()\n",
        "    plt.title(f\"Batch Size: {batch_size}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        " \n",
        "\n"
      ],
      "metadata": {
        "id": "HcCVuUfDAn3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 4 - Adding Regularisation\n",
        "\n",
        "The model as provided does not contain any regularisation techniques. Edit the model architecture to include at least two examples of regularisation. Retrain the model using the new architecture, and repeat the analysis performed in task 1.\n",
        "\n",
        "In your report, compare and contrast the results from this task, with those from the initial model configuration. Explain HOW and WHY the results are different, with consideration to the predicted classifications, losses and metrics."
      ],
      "metadata": {
        "id": "fqZH46DmBSXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Update the model architecture to include at least two types of regularization.\n",
        "### Train the model using the ideal settings found in previous tasks.\n"
      ],
      "metadata": {
        "id": "AiLBxBQ3BvqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Repeat your analysis from task 1, creating plots of the losses, metrics AND\n",
        "### predicted classifications of images in the test set.\n"
      ],
      "metadata": {
        "id": "70TKDDKcB681"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}